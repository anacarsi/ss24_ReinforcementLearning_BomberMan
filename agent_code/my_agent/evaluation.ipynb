{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from utils import get_settings_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_settings_dir()\n",
    "from settings import ROWS, COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code for Q-learning agent\n",
    "\n",
    "# -------------------- Setup Callbacks (callbacks.py) --------------------\n",
    "\n",
    "# Simulating the Q-learning agent callbacks and training environment\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from callbacks import QLearningModel, state_to_features\n",
    "\n",
    "model = QLearningModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Setup Training (train.py) --------------------\n",
    "from typing import List, Tuple\n",
    "from objective import Objective\n",
    "import numpy as np\n",
    "\n",
    "class AgentTraining:\n",
    "    def __init__(self):\n",
    "        self.episodes = 1000\n",
    "        self.max_steps = 100\n",
    "        self.rewards = []\n",
    "        self.eps = 0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.q_table = {}\n",
    "        self.graph = []\n",
    "        self.epsilon = 1.0\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99\n",
    "        self.model = QLearningModel()\n",
    "        self.actions = self.model.actions\n",
    "        self.objective = Objective(task=1, game_state={'self': (None, None, None, (7, 7)),  # Player's current state (self_position)\n",
    "                                                        'coins': [(5, 5), (2, 8)],            # List of coin positions\n",
    "                                                        'field': np.zeros((ROWS, COLS))      # Game field representation (grid)\n",
    "                                                        })\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def game_events_occurred(self, old_game_state, action, new_game_state, events):\n",
    "        \"\"\" Update Q-table based on events \"\"\"\n",
    "        state = state_to_features(old_game_state)\n",
    "        new_state = state_to_features(new_game_state)\n",
    "\n",
    "        # Extract necessary parameters for reward calculation\n",
    "        old_player_pos = old_game_state['self'][3]\n",
    "        old_objective = old_game_state['coins'][0]  # Assuming coins are the objective\n",
    "        new_self_pos = new_game_state['self'][3]\n",
    "        new_objective = new_game_state['coins'][0]\n",
    "        old_field = old_game_state['field']\n",
    "        new_field = new_game_state['field']\n",
    "\n",
    "        reward = self.calculate_reward(\n",
    "            old_player_pos, old_objective, new_self_pos, new_objective, old_field, new_field, events\n",
    "        )\n",
    "        self.update_q_table(state, action, reward, new_state)\n",
    "\n",
    "    def calculate_reward(\n",
    "        self, \n",
    "        old_player_pos: Tuple[int, int], old_objective: Tuple[int, int],\n",
    "        new_self_pos: Tuple[int, int], new_objective: Tuple[int, int],\n",
    "        old_field: np.ndarray, new_field: np.ndarray,\n",
    "        events: List[str]\n",
    "        ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the state and events.\n",
    "        \"\"\"\n",
    "        old_distance = self.objective.distance_objective(start_pos=old_player_pos, objective_pos=old_objective, field=old_field)\n",
    "        new_distance = self.objective.distance_objective(start_pos=new_self_pos, objective_pos=new_objective, field=new_field)\n",
    "\n",
    "        if old_objective == new_objective:\n",
    "            reward = 1 if len(new_distance) < len(old_distance) else -3  # discourages moving away or not making progress\n",
    "        else: \n",
    "            reward = 0  # no reward if objective has changed\n",
    "        \n",
    "        if \"COIN_COLLECTED\" in events:\n",
    "            reward += 2\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\" Update Q-table using Q-learning formula \"\"\"\n",
    "        state_tuple = tuple(state)\n",
    "        if state_tuple not in self.q_table:\n",
    "            self.q_table[state_tuple] = np.zeros(len(self.actions))\n",
    "        \n",
    "        next_state_tuple = tuple(next_state)\n",
    "        if next_state_tuple not in self.q_table:\n",
    "            self.q_table[next_state_tuple] = np.zeros(len(self.actions))\n",
    "\n",
    "        action_index = self.actions.index(action)\n",
    "        \n",
    "        # Find the max Q-value for the next state (future reward estimation)\n",
    "        max_next_q_value = np.max(self.q_table[next_state_tuple])\n",
    "\n",
    "        # Q-learning update rule\n",
    "        current_q_value = self.q_table[state_tuple][action_index]\n",
    "        self.q_table[state_tuple][action_index] = current_q_value + self.learning_rate * (\n",
    "            reward + self.discount_factor * max_next_q_value - current_q_value\n",
    "        )\n",
    "\n",
    "    def end_of_round(self, last_game_state, last_action, events):\n",
    "        \"\"\" Handle end of round logic \"\"\"\n",
    "        reward = -100 if 'KILLED_SELF' in events else 0\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.graph.append((self.eps, self.total_reward))\n",
    "        self.total_reward = 0\n",
    "\n",
    "        if self.eps % 1000 == 0:\n",
    "            self.save_metrics()\n",
    "        \n",
    "        self.eps += 1\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.log_progress(self.eps)\n",
    "\n",
    "\n",
    "    def save_metrics(self):\n",
    "        \"\"\" Save training metrics (e.g., Q-table, graph) \"\"\"\n",
    "        with open(\"q_table.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "        with open(\"graph.txt\", \"w\") as f:\n",
    "            f.write(str(self.rewards))\n",
    "            \n",
    "    \n",
    "    def log_progress(self, episode):\n",
    "        \"\"\" Log training progress \"\"\"\n",
    "        if len(self.rewards) >= 100:\n",
    "            avg_reward = sum(self.rewards[-100:]) / 100\n",
    "        elif self.rewards:\n",
    "            avg_reward = sum(self.rewards) / len(self.rewards)\n",
    "        else:\n",
    "            avg_reward = 0  # Handle case where self.rewards is empty\n",
    "\n",
    "        print(f\"Episode {episode}: Average Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Average Reward: 0\n",
      "Episode 0: Average Reward: 0\n",
      "Episode 1: Average Reward: 0\n",
      "Episode 2: Average Reward: 0\n",
      "Episode 3: Average Reward: 0\n",
      "Episode 4: Average Reward: 0\n",
      "Episode 5: Average Reward: 0\n",
      "Episode 6: Average Reward: 0\n",
      "Episode 7: Average Reward: 0\n",
      "Episode 8: Average Reward: 0\n",
      "Episode 9: Average Reward: 0\n",
      "Q-Table:\n",
      "{(7, 7, 1, 0): array([0.19 , 0.3  , 0.271, 0.271, 0.1  ]), (6, 6, 1, 0): array([0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Test Execution --------------------\n",
    "\n",
    "# Create instances of the agent and model\n",
    "q_model = QLearningModel()\n",
    "trainer = AgentTraining()\n",
    "\n",
    "# Simulate a game loop\n",
    "for episode in range(10):  # Run a limited number of episodes for testing\n",
    "    old_game_state = {'self': (None, None, None, (7, 7)), 'coins': [(5, 5)], 'field': np.zeros((10, 10))}\n",
    "    new_game_state = {'self': (None, None, None, (6, 6)), 'coins': [(5, 5)], 'field': np.zeros((10, 10))}\n",
    "    state = state_to_features(old_game_state)\n",
    "    \n",
    "    # Choose an action using the Q-learning model\n",
    "    action = q_model.choose_action(state)\n",
    "\n",
    "    # Simulate events for the action\n",
    "    events = [\"COIN_COLLECTED\"] if action == 'RIGHT' else []\n",
    "    \n",
    "    # Training step\n",
    "    trainer.game_events_occurred(old_game_state, action, new_game_state, events)\n",
    "    \n",
    "    # End of round logic after some steps\n",
    "    if episode % 10 == 0:\n",
    "        trainer.end_of_round(new_game_state, action, events)\n",
    "\n",
    "    # Log progress\n",
    "    trainer.log_progress(episode)\n",
    "\n",
    "# Print the Q-table for review\n",
    "print(\"Q-Table:\")\n",
    "print(trainer.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque # self.transitions stores recent transitions (state, action, reward, next_state) using a deque\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "import events as e\n",
    "\n",
    "# Define a namedtuple for transitions (state, action, next_state, reward)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Hyperparameters\n",
    "TRANSITION_HISTORY_SIZE = 3  # Number of transitions to keep in memory\n",
    "DISCOUNT_FACTOR = 0.99  # Gamma for future rewards\n",
    "LEARNING_RATE = 0.1  # Alpha for Q-learning updates\n",
    "\n",
    "# Example custom event\n",
    "PLACEHOLDER_EVENT = \"PLACEHOLDER\"\n",
    "\n",
    "def setup_training(self):\n",
    "    \"\"\"\n",
    "    Initialize the agent for training.\n",
    "    This method is called after `setup` in callbacks.py.\n",
    "    \"\"\"\n",
    "    # Setup a deque to store transitions with a limited history size\n",
    "    self.transitions = deque(maxlen=TRANSITION_HISTORY_SIZE)\n",
    "    # Initialize training parameters\n",
    "    self.epsilon = 1.0  # Exploration rate\n",
    "    self.epsilon_min = 0.1\n",
    "    self.epsilon_decay = 0.995\n",
    "    self.q_table = np.zeros((100, 6))  # Example: assuming a 100 state x 6 action Q-table\n",
    "    self.graph = []  # To store metrics (for example, rewards over episodes)\n",
    "\n",
    "def game_events_occurred(self, old_game_state: dict, self_action: str, new_game_state: dict, events: List[str]):\n",
    "    \"\"\"\n",
    "    Called once per step to allow intermediate rewards based on game events.\n",
    "    \n",
    "    Update the agent's Q-table or policy based on game events that occurred during the step.\n",
    "    \"\"\"\n",
    "    self.logger.debug(f'Encountered game event(s): {\", \".join(map(repr, events))} in step {new_game_state[\"step\"]}')\n",
    "    \n",
    "    # Convert states to features using the provided helper function\n",
    "    old_state = state_to_features(old_game_state)\n",
    "    new_state = state_to_features(new_game_state)\n",
    "    \n",
    "    # Calculate reward from events\n",
    "    reward = reward_from_events(self, events)\n",
    "    \n",
    "    # Append the transition (state, action, reward, next_state) to the deque\n",
    "    self.transitions.append(Transition(old_state, self_action, new_state, reward))\n",
    "    \n",
    "    # Update Q-table with the new transition\n",
    "    update_q_table(self, old_state, self_action, reward, new_state)\n",
    "\n",
    "def end_of_round(self, last_game_state: dict, last_action: str, events: List[str]):\n",
    "    \"\"\"\n",
    "    Called at the end of each game or when the agent died to finalize rewards and update the model.\n",
    "    \n",
    "    This method is also responsible for saving the agent's model periodically.\n",
    "    \"\"\"\n",
    "    self.logger.debug(f'Encountered event(s): {\", \".join(map(repr, events))} in the final step')\n",
    "    \n",
    "    # Add the final transition with the last state and action\n",
    "    last_state = state_to_features(last_game_state)\n",
    "    final_reward = reward_from_events(self, events)\n",
    "    self.transitions.append(Transition(last_state, last_action, None, final_reward))\n",
    "    \n",
    "    # Save the model periodically or after a set number of rounds\n",
    "    if self.eps % 100 == 0:  # Example: save every 100 episodes\n",
    "        with open(\"my-saved-model.pt\", \"wb\") as file:\n",
    "            pickle.dump(self.q_table, file)\n",
    "    \n",
    "    # Increment the episode count and update epsilon for exploration/exploitation\n",
    "    self.eps += 1\n",
    "    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "def reward_from_events(self, events: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Calculate reward from the game events that occurred during a step.\n",
    "    \n",
    "    Customize rewards to encourage or discourage specific agent behaviors.\n",
    "    \"\"\"\n",
    "    game_rewards = {\n",
    "        e.COIN_COLLECTED: 1,\n",
    "        e.KILLED_OPPONENT: 5,\n",
    "        PLACEHOLDER_EVENT: -0.1  # Negative reward for a custom placeholder event\n",
    "    }\n",
    "    \n",
    "    # Sum up the rewards from the events\n",
    "    reward_sum = 0\n",
    "    for event in events:\n",
    "        if event in game_rewards:\n",
    "            reward_sum += game_rewards[event]\n",
    "    self.logger.info(f\"Awarded {reward_sum} for events {', '.join(events)}\")\n",
    "    return reward_sum\n",
    "\n",
    "def update_q_table(self, state, action, reward, next_state):\n",
    "    \"\"\"\n",
    "    Update the Q-table using the Q-learning update rule.\n",
    "    \n",
    "    :param state: The current state in features.\n",
    "    :param action: The action taken.\n",
    "    :param reward: The reward received from the action.\n",
    "    :param next_state: The next state after the action.\n",
    "    \"\"\"\n",
    "    # Find the max Q-value for the next state (future reward estimation)\n",
    "    if next_state is not None:\n",
    "        max_next_q_value = np.max(self.q_table[next_state])\n",
    "    else:\n",
    "        max_next_q_value = 0  # No next state at the end of the game\n",
    "\n",
    "    # Get the current Q-value for the (state, action) pair\n",
    "    current_q_value = self.q_table[state, action]\n",
    "    \n",
    "    # Q-learning update rule\n",
    "    self.q_table[state, action] = current_q_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * max_next_q_value - current_q_value)\n",
    "\n",
    "def log_training_progress(self, episode):\n",
    "    \"\"\"\n",
    "    Log the agent's training progress, e.g., the average reward over recent episodes.\n",
    "    \n",
    "    :param episode: The current episode number.\n",
    "    \"\"\"\n",
    "    # Example: Log the average reward over the last 100 episodes\n",
    "    avg_reward = sum(self.rewards[-100:]) / 100 if len(self.rewards) >= 100 else sum(self.rewards) / len(self.rewards)\n",
    "    self.logger.info(f\"Episode {episode}: Average Reward: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awarded 1 for events COIN_COLLECTED\n",
      "self_action: RIGHT\n",
      "Awarded 1 for events COIN_COLLECTED\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "import random\n",
    "\n",
    "# Define namedtuple for transitions\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Define the Agent class\n",
    "class MyAgent:\n",
    "    # Hyperparameters\n",
    "    TRANSITION_HISTORY_SIZE = 3\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    LEARNING_RATE = 0.1\n",
    "\n",
    "    # Initialize the agent\n",
    "    def __init__(self):\n",
    "        self.setup_training()\n",
    "    \n",
    "    def setup_training(self):\n",
    "        \"\"\"Initialize the agent for training.\"\"\"\n",
    "        self.transitions = deque(maxlen=self.TRANSITION_HISTORY_SIZE)\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.q_table = np.zeros((100, 6))  # Example Q-table size\n",
    "        self.graph = []\n",
    "        self.eps = 0\n",
    "        self.rewards = []\n",
    "        self.logger = lambda x: print(x)  # Simple logger function\n",
    "\n",
    "    def action_to_index(self, action: str) -> int:\n",
    "        action_map = {\"UP\": 0, \"RIGHT\": 1, \"DOWN\": 2, \"LEFT\": 3, \"BOMB\": 4, \"WAIT\": 5}\n",
    "        return action_map.get(action, -1)  # Return -1 for unknown actions\n",
    "    \n",
    "    def game_events_occurred(self, old_game_state: Dict, self_action: str, new_game_state: Dict, events: List[str]):\n",
    "        \"\"\"Handle game events and update Q-table.\"\"\"\n",
    "        old_state = state_to_features(old_game_state)\n",
    "        new_state = state_to_features(new_game_state)\n",
    "        reward = self.reward_from_events(events)\n",
    "        \n",
    "        # Convert action to an index\n",
    "        print(f'self_action: {self_action}')\n",
    "        action_index = self.action_to_index(self_action)\n",
    "        \n",
    "        # Append the transition (state, action, reward, next_state) to the deque\n",
    "        self.transitions.append(Transition(old_state, action_index, new_state, reward))\n",
    "        \n",
    "        # Update Q-table with the new transition\n",
    "        self.update_q_table(old_state, action_index, reward, new_state)\n",
    "\n",
    "    def end_of_round(self, last_game_state: Dict, last_action: str, events: List[str]):\n",
    "        \"\"\"Finalize rewards and update model at the end of each round.\"\"\"\n",
    "        last_state = state_to_features(last_game_state)\n",
    "        final_reward = self.reward_from_events(events)\n",
    "        self.transitions.append(Transition(last_state, last_action, None, final_reward))\n",
    "        \n",
    "        if self.eps % 100 == 0:\n",
    "            with open(\"my-saved-model.pt\", \"wb\") as file:\n",
    "                pickle.dump(self.q_table, file)\n",
    "        \n",
    "        self.eps += 1\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def reward_from_events(self, events: List[str]) -> int:\n",
    "        \"\"\"Calculate reward based on events.\"\"\"\n",
    "        game_rewards = {\n",
    "            \"COIN_COLLECTED\": 1,\n",
    "            \"KILLED_OPPONENT\": 5,\n",
    "            \"PLACEHOLDER_EVENT\": -0.1\n",
    "        }\n",
    "        reward_sum = 0\n",
    "        for event in events:\n",
    "            if event in game_rewards:\n",
    "                reward_sum += game_rewards[event]\n",
    "        self.logger(f\"Awarded {reward_sum} for events {', '.join(events)}\")\n",
    "        return reward_sum\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table using the Q-learning rule.\"\"\"\n",
    "        if next_state is not None:\n",
    "            max_next_q_value = np.max(self.q_table[next_state])\n",
    "        else:\n",
    "            max_next_q_value = 0\n",
    "\n",
    "        current_q_value = self.q_table[state, action]\n",
    "        self.q_table[state, action] = current_q_value + self.LEARNING_RATE * (reward + self.DISCOUNT_FACTOR * max_next_q_value - current_q_value)\n",
    "    \n",
    "    def log_training_progress(self, episode):\n",
    "        \"\"\"Log training progress.\"\"\"\n",
    "        avg_reward = sum(self.rewards[-100:]) / 100 if len(self.rewards) >= 100 else sum(self.rewards) / len(self.rewards)\n",
    "        self.logger(f\"Episode {episode}: Average Reward: {avg_reward}\")\n",
    "\n",
    "# Helper function to simulate state_to_features (Replace with actual implementation)\n",
    "def state_to_features(game_state):\n",
    "    \"\"\"Convert game state to feature indices.\"\"\"\n",
    "    # For simplicity, this example assumes a very basic encoding\n",
    "    # In a real scenario, this should encode the game state appropriately\n",
    "    return int(game_state.get('self', [0, 0, 0, 0])[3][0] * 10 + game_state.get('self', [0, 0, 0, 0])[3][1])\n",
    "\n",
    "\n",
    "# Mock data for testing\n",
    "old_game_state = {'self': [0, 0, 0, (0, 0)], 'coins': [(1, 1)], 'field': np.zeros((10, 10))}\n",
    "new_game_state = {'self': [0, 0, 0, (1, 1)], 'coins': [(1, 1)], 'field': np.zeros((10, 10))}\n",
    "events = [\"COIN_COLLECTED\"]\n",
    "\n",
    "# Create an instance of MyAgent\n",
    "agent = MyAgent()\n",
    "\n",
    "# Test agent methods\n",
    "agent.game_events_occurred(old_game_state, \"RIGHT\", new_game_state, events)\n",
    "agent.end_of_round(last_game_state=new_game_state, last_action=\"RIGHT\", events=events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with symmetry version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
