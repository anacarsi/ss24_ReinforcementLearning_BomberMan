{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from utils import get_settings_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_settings_dir()\n",
    "from settings import ROWS, COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ssetup code for Q-learning agent\n",
    "\n",
    "# -------------------- Setup Callbacks (callbacks.py) --------------------\n",
    "\n",
    "# Simulating the Q-learning agent callbacks and training environment\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from callbacks import QLearningModel, state_to_features\n",
    "\n",
    "model = QLearningModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Setup Training (train.py) --------------------\n",
    "from typing import List, Tuple\n",
    "from objective import Objective\n",
    "import numpy as np\n",
    "\n",
    "class AgentTraining:\n",
    "    def __init__(self):\n",
    "        self.episodes = 1000\n",
    "        self.max_steps = 100\n",
    "        self.rewards = []\n",
    "        self.eps = 0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.q_table = {}\n",
    "        self.graph = []\n",
    "        self.epsilon = 1.0\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99\n",
    "        self.model = QLearningModel()\n",
    "        self.actions = self.model.actions\n",
    "        self.objective = Objective(task=1, game_state={'self': (None, None, None, (7, 7)),  # Player's current state (self_position)\n",
    "                                                        'coins': [(5, 5), (2, 8)],            # List of coin positions\n",
    "                                                        'field': np.zeros((ROWS, COLS))      # Game field representation (grid)\n",
    "                                                        })\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def game_events_occurred(self, old_game_state, action, new_game_state, events):\n",
    "        \"\"\" Update Q-table based on events \"\"\"\n",
    "        state = state_to_features(old_game_state)\n",
    "        new_state = state_to_features(new_game_state)\n",
    "\n",
    "        # Extract necessary parameters for reward calculation\n",
    "        old_player_pos = old_game_state['self'][3]\n",
    "        old_objective = old_game_state['coins'][0]  # Assuming coins are the objective\n",
    "        new_self_pos = new_game_state['self'][3]\n",
    "        new_objective = new_game_state['coins'][0]\n",
    "        old_field = old_game_state['field']\n",
    "        new_field = new_game_state['field']\n",
    "\n",
    "        reward = self.calculate_reward(\n",
    "            old_player_pos, old_objective, new_self_pos, new_objective, old_field, new_field, events\n",
    "        )\n",
    "        self.update_q_table(state, action, reward, new_state)\n",
    "\n",
    "    def calculate_reward(\n",
    "        self, \n",
    "        old_player_pos: Tuple[int, int], old_objective: Tuple[int, int],\n",
    "        new_self_pos: Tuple[int, int], new_objective: Tuple[int, int],\n",
    "        old_field: np.ndarray, new_field: np.ndarray,\n",
    "        events: List[str]\n",
    "        ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the state and events.\n",
    "        \"\"\"\n",
    "        old_distance = self.objective.distance_objective(start_pos=old_player_pos, objective_pos=old_objective, field=old_field)\n",
    "        new_distance = self.objective.distance_objective(start_pos=new_self_pos, objective_pos=new_objective, field=new_field)\n",
    "\n",
    "        if old_objective == new_objective:\n",
    "            reward = 1 if len(new_distance) < len(old_distance) else -3  # discourages moving away or not making progress\n",
    "        else: \n",
    "            reward = 0  # no reward if objective has changed\n",
    "        \n",
    "        if \"COIN_COLLECTED\" in events:\n",
    "            reward += 2\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\" Update Q-table using Q-learning formula \"\"\"\n",
    "        state_tuple = tuple(state)\n",
    "        if state_tuple not in self.q_table:\n",
    "            self.q_table[state_tuple] = np.zeros(len(self.actions))\n",
    "        \n",
    "        next_state_tuple = tuple(next_state)\n",
    "        if next_state_tuple not in self.q_table:\n",
    "            self.q_table[next_state_tuple] = np.zeros(len(self.actions))\n",
    "\n",
    "        action_index = self.actions.index(action)\n",
    "        \n",
    "        # Find the max Q-value for the next state (future reward estimation)\n",
    "        max_next_q_value = np.max(self.q_table[next_state_tuple])\n",
    "\n",
    "        # Q-learning update rule\n",
    "        current_q_value = self.q_table[state_tuple][action_index]\n",
    "        self.q_table[state_tuple][action_index] = current_q_value + self.learning_rate * (\n",
    "            reward + self.discount_factor * max_next_q_value - current_q_value\n",
    "        )\n",
    "\n",
    "    def end_of_round(self, last_game_state, last_action, events):\n",
    "        \"\"\" Handle end of round logic \"\"\"\n",
    "        reward = -100 if 'KILLED_SELF' in events else 0\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.graph.append((self.eps, self.total_reward))\n",
    "        self.total_reward = 0\n",
    "\n",
    "        if self.eps % 1000 == 0:\n",
    "            self.save_metrics()\n",
    "        \n",
    "        self.eps += 1\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.log_progress(self.eps)\n",
    "\n",
    "\n",
    "    def save_metrics(self):\n",
    "        \"\"\" Save training metrics (e.g., Q-table, graph) \"\"\"\n",
    "        with open(\"q_table.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "        with open(\"graph.txt\", \"w\") as f:\n",
    "            f.write(str(self.rewards))\n",
    "            \n",
    "    \n",
    "    def log_progress(self, episode):\n",
    "        \"\"\" Log training progress \"\"\"\n",
    "        if len(self.rewards) >= 100:\n",
    "            avg_reward = sum(self.rewards[-100:]) / 100\n",
    "        elif self.rewards:\n",
    "            avg_reward = sum(self.rewards) / len(self.rewards)\n",
    "        else:\n",
    "            avg_reward = 0  # Handle case where self.rewards is empty\n",
    "\n",
    "        print(f\"Episode {episode}: Average Reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# End of round logic after some steps\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 24\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mend_of_round(new_game_state, action, events)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Log progress\u001b[39;00m\n\u001b[0;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_progress(episode)\n",
      "Cell \u001b[1;32mIn[18], line 102\u001b[0m, in \u001b[0;36mAgentTraining.end_of_round\u001b[1;34m(self, last_game_state, last_action, events)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_progress(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "Cell \u001b[1;32mIn[18], line 115\u001b[0m, in \u001b[0;36mAgentTraining.log_progress\u001b[1;34m(self, episode)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_progress\u001b[39m(\u001b[38;5;28mself\u001b[39m, episode):\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Log training progress \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     avg_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Average Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# -------------------- Test Execution --------------------\n",
    "\n",
    "# Create instances of the agent and model\n",
    "q_model = QLearningModel()\n",
    "trainer = AgentTraining()\n",
    "\n",
    "# Simulate a game loop\n",
    "for episode in range(10):  # Run a limited number of episodes for testing\n",
    "    old_game_state = {'self': (None, None, None, (7, 7)), 'coins': [(5, 5)], 'field': np.zeros((10, 10))}\n",
    "    new_game_state = {'self': (None, None, None, (6, 6)), 'coins': [(5, 5)], 'field': np.zeros((10, 10))}\n",
    "    state = state_to_features(old_game_state)\n",
    "    \n",
    "    # Choose an action using the Q-learning model\n",
    "    action = q_model.choose_action(state)\n",
    "\n",
    "    # Simulate events for the action\n",
    "    events = [\"COIN_COLLECTED\"] if action == 'RIGHT' else []\n",
    "    \n",
    "    # Training step\n",
    "    trainer.game_events_occurred(old_game_state, action, new_game_state, events)\n",
    "    \n",
    "    # End of round logic after some steps\n",
    "    if episode % 10 == 0:\n",
    "        trainer.end_of_round(new_game_state, action, events)\n",
    "\n",
    "    # Log progress\n",
    "    trainer.log_progress(episode)\n",
    "\n",
    "# Print the Q-table for review\n",
    "print(\"Q-Table:\")\n",
    "print(trainer.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with symmetry version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
